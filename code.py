# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sih5FAwonnpcrykKOW77NF7fD9Rjh-Of
"""

import re
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import string
from tensorflow.keras.optimizers import AdamW
from transformers import RobertaTokenizer, RobertaForSequenceClassification, get_linear_schedule_with_warmup
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
import warnings
warnings.filterwarnings('ignore')
import logging
logging.basicConfig(level=logging.ERROR)

from google.colab import drive
drive.mount('/content/drive')

csv_path = "/content/drive/MyDrive/Colab Notebooks/data.csv"
df =  pd.read_csv(csv_path, encoding="ISO-8859-1", header=None , names=['label', 'ids', 'data', 'flag' , 'user','sentence'])

print('Number of training sentences: {:,}\n'.format(df.shape[0]))
df.sample(10)

#Data Preprocessing
df = df[['label', 'sentence']]
df['label'] = df['label'].replace({4: 1})
df = df.dropna(subset=['sentence']) # Remove rows with missing 'sentence'

def clean_tweet(tweet):
    if not isinstance(tweet, str):
        return ""  # Or some other default value

    # Convert to lowercase
    tweet = tweet.lower()

    # Replace all URLs with ''
    tweet = re.sub(r"((http://)[^ ]*|(https://)[^ ]*|(www\.)[^ ]*)", '', tweet)

    # Replace @USERNAME with ''
    tweet = re.sub(r'@[^\s]+', '', tweet)

    # Replace 3 or more consecutive letters by 2 letters
    tweet = re.sub(r"(.)\1\1+", r"\1\1", tweet)

    # Adding space on either side of '/' to separate words
    tweet = re.sub(r'/', ' / ', tweet)

    # Remove punctuations, links, mentions, and \r\n new line characters
    tweet = tweet.replace('\r', '').replace('\n', ' ').replace('\n', ' ').lower()
    tweet = re.sub(r"(?:\@|https?\://)\S+", "", tweet)
    tweet = re.sub(r'[^\x00-\x7f]', r'', tweet)
    banned_list = string.punctuation + 'Ã' + '±' + 'ã' + '¼' + 'â' + '»' + '§'
    table = str.maketrans('', '', banned_list)
    tweet = tweet.translate(table)

    # Clean hashtags
    tweet = " ".join(word.strip() for word in re.split('#(?!(?:hashtag)\b)[\w-]+(?=(?:\s+#[\w-]+)*\s*$)', tweet))
    tweet = " ".join(word.strip() for word in re.split('#|_', tweet))

    # Filter special characters such as & and $
    tweet = ' '.join([word if ('$' not in word) and ('&' not in word) else '' for word in tweet.split(' ')])

    # Remove multiple spaces
    tweet = re.sub(r"\s\s+", " ", tweet)

    return tweet.strip()

df['preprocessing_sentence'] = df['sentence'].apply(clean_tweet)
df = df[df['preprocessing_sentence'].str.split().str.len() >= 5].reset_index(drop=True)

# Balance the dataset
label_counts = df['label'].value_counts()
min_count = min(label_counts)
balanced_df = pd.concat([df[df['label'] == label].sample(min_count, random_state=42)
                        for label in df['label'].unique()])
balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle
df = balanced_df

# Tokenization and Input Encoding
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
max_len = 128  # Choose an appropriate max length

def encode_data(texts, labels, tokenizer, max_len):
    input_ids = []
    attention_masks = []

    for text in texts:
        encoded_dict = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=max_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'  # Return PyTorch tensors
        )
        input_ids.append(encoded_dict['input_ids'])
        attention_masks.append(encoded_dict['attention_mask'])

    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    labels = torch.tensor(labels)

    return input_ids, attention_masks, labels

input_ids, attention_masks, labels = encode_data(
    df['preprocessing_sentence'].tolist(),
    df['label'].tolist(),
    tokenizer,
    max_len
)

# Train-Test Split
dataset = TensorDataset(input_ids, attention_masks, labels)
train_size = int(0.9 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

batch_size = 16  # Adjust as needed

train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)
val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)

# Model Initialization
model = RobertaForSequenceClassification.from_pretrained(
    'roberta-base',
    num_labels=2,  # Binary classification
    output_attentions=False,
    output_hidden_states=False,
)

from torch.optim import AdamW  # Try this alternative

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Optimizer and Scheduler
optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)
epochs = 3  # Or more
total_steps = len(train_dataloader) * epochs
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

# Training Loop
def train(model, train_dataloader, val_dataloader, optimizer, scheduler, device, epochs=3):
    for epoch_i in range(0, epochs):
        print(f'======== Epoch {epoch_i + 1} / {epochs} ========')
        print('Training...')

        model.train()
        total_train_loss = 0

        for step, batch in enumerate(train_dataloader):
            b_input_ids = batch[0].to(device)
            b_input_mask = batch[1].to(device)
            b_labels = batch[2].to(device)

            model.zero_grad()
            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)
            loss = outputs.loss
            total_train_loss += loss.item()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping
            optimizer.step()
            scheduler.step()

            if step % 400 == 0 and not step == 0:
                print(f'  Batch {step:>5,}  of  {len(train_dataloader):,}    Loss: {loss.item():.4f}')

        avg_train_loss = total_train_loss / len(train_dataloader)
        print(f'  Average training loss: {avg_train_loss:.4f}')

        # Validation
        print('\nValidating...')
        model.eval()
        total_val_loss = 0
        all_preds = []
        all_labels = []

        for batch in val_dataloader:
            b_input_ids = batch[0].to(device)
            b_input_mask = batch[1].to(device)
            b_labels = batch[2].to(device)

            with torch.no_grad():
                outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)
            loss = outputs.loss
            total_val_loss += loss.item()

            logits = outputs.logits
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            labels = b_labels.cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels)

        avg_val_loss = total_val_loss / len(val_dataloader)
        print(f'  Average validation loss: {avg_val_loss:.4f}')

        print('  Validation Classification Report:')
        print(classification_report(all_labels, all_preds))

    print('\nTraining complete!')

train(model, train_dataloader, val_dataloader, optimizer, scheduler, device, epochs)


# Save the trained model (optional)
# model_save_path = '/content/drive/MyDrive/Colab Notebooks/roberta_sentiment_model'
# model.save_pretrained(model_save_path)
# tokenizer.save_pretrained(model_save_path)